{
  "name": "FRED_Release_Ingestion",
  "properties": {
    "nbformat": 4,
    "nbformat_minor": 2,
    "bigDataPool": {
      "referenceName": "spack2",
      "type": "BigDataPoolReference"
    },
    "sessionProperties": {
      "driverMemory": "56g",
      "driverCores": 8,
      "executorMemory": "56g",
      "executorCores": 8,
      "numExecutors": 2,
      "runAsWorkspaceSystemIdentity": false,
      "conf": {
        "spark.dynamicAllocation.enabled": "false",
        "spark.dynamicAllocation.minExecutors": "2",
        "spark.dynamicAllocation.maxExecutors": "2"
      }
    },
    "metadata": {
      "saveOutput": true,
      "enableDebugMode": false,
      "kernelspec": {
        "name": "synapse_pyspark",
        "display_name": "Synapse PySpark"
      },
      "language_info": {
        "name": "python"
      },
      "a365ComputeOptions": {
        "id": "/subscriptions/6f928fec-8d15-47d7-b27b-be8b568e9789/resourceGroups/MTWS_Synapse/providers/Microsoft.Synapse/workspaces/externaldata/bigDataPools/spack2",
        "name": "spack2",
        "type": "Spark",
        "endpoint": "https://externaldata.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spack2",
        "auth": {
          "type": "AAD",
          "authResource": "https://dev.azuresynapse.net"
        },
        "sparkVersion": "3.3",
        "nodeCount": 3,
        "cores": 8,
        "memory": 56
      },
      "sessionKeepAliveTimeout": 30
    },
    "cells": [
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "#!/usr/bin/env python3\n",
          "\"\"\"\n",
          "FRED RELEASE DATA INGESTION TO ADX\n",
          "Generic ingestion for any FRED release with proper pagination and error handling\n",
          "\"\"\"\n",
          "\n",
          "import json\n",
          "import time\n",
          "import requests\n",
          "import sys\n",
          "import os\n",
          "from datetime import datetime, timezone\n",
          "from pyspark.sql import SparkSession\n",
          "from pyspark.sql.functions import col, lit, to_timestamp, to_date, current_timestamp\n",
          "from pyspark.sql.types import *\n",
          "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
          "import traceback\n",
          "\n",
          "print(\"=\" * 80)\n",
          "print(\"FRED RELEASE DATA INGESTION TO ADX\")\n",
          "print(\"=\" * 80)\n",
          "print(f\"Execution Time: {datetime.now(timezone.utc).isoformat()}\")\n",
          "print(f\"Spark Version: {spark.version}\")\n",
          "\n",
          "# Parameter handling for Synapse pipeline\n",
          "release_id = None\n",
          "try:\n",
          "    # Synapse pipeline parameter via dbutils\n",
          "    dbutils.widgets.text(\"release_id\", \"10\")\n",
          "    release_id = dbutils.widgets.get(\"release_id\")\n",
          "    print(f\"✅ Release ID from pipeline: {release_id}\")\n",
          "except NameError:\n",
          "    # Fallback for local testing\n",
          "    release_id = os.environ.get('RELEASE_ID', '10')\n",
          "    print(f\"✅ Release ID from environment: {release_id}\")\n",
          "\n",
          "release_id = str(release_id)\n",
          "print(f\"\\nProcessing Release ID: {release_id}\")\n",
          "\n",
          "# API Keys\n",
          "api_keys = [\n",
          "    \"300a685dde47306828b8ff947a527113\",\n",
          "    \"1211005324f682746cb9b44882837c7b\",\n",
          "    \"ad3badb0ffdaaef1a205e105a14c0fbf\",\n",
          "    \"21acd97c4988e53af02e98587d5424d0\"\n",
          "]\n",
          "\n",
          "# Configuration\n",
          "FRED_BASE_URL = \"https://api.stlouisfed.org/fred\"\n",
          "BATCH_LIMIT = 1000  # FRED API maximum per request\n",
          "MAX_WORKERS = 4\n",
          "\n",
          "# Step 1: Get release info\n",
          "print(\"\\n\" + \"=\" * 40)\n",
          "print(\"STEP 1: FETCHING RELEASE INFO\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "release_url = f\"{FRED_BASE_URL}/release\"\n",
          "release_params = {\n",
          "    \"release_id\": release_id,\n",
          "    \"api_key\": api_keys[0],\n",
          "    \"file_type\": \"json\"\n",
          "}\n",
          "\n",
          "try:\n",
          "    response = requests.get(release_url, params=release_params, timeout=30)\n",
          "    response.raise_for_status()\n",
          "    release_data = response.json()\n",
          "    release_info = release_data['releases'][0]\n",
          "    release_name = release_info.get('name', f'Release_{release_id}')\n",
          "    print(f\"✅ Release: {release_name}\")\n",
          "except Exception as e:\n",
          "    print(f\"⚠️  Error getting release info: {e}\")\n",
          "    release_name = f\"Release_{release_id}\"\n",
          "\n",
          "# Step 2: Fetch all series with pagination\n",
          "print(\"\\n\" + \"=\" * 40)\n",
          "print(\"STEP 2: FETCHING SERIES\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "all_series = []\n",
          "offset = 0\n",
          "batch_count = 0\n",
          "\n",
          "while batch_count < 50:  # Safety limit\n",
          "    series_url = f\"{FRED_BASE_URL}/release/series\"\n",
          "    params = {\n",
          "        \"release_id\": release_id,\n",
          "        \"api_key\": api_keys[batch_count % len(api_keys)],\n",
          "        \"file_type\": \"json\",\n",
          "        \"limit\": BATCH_LIMIT,\n",
          "        \"offset\": offset\n",
          "    }\n",
          "    \n",
          "    print(f\"Fetching batch {batch_count + 1} (offset: {offset})...\")\n",
          "    \n",
          "    try:\n",
          "        response = requests.get(series_url, params=params, timeout=30)\n",
          "        if response.status_code == 400:\n",
          "            print(f\"Reached end of data\")\n",
          "            break\n",
          "        response.raise_for_status()\n",
          "        data = response.json()\n",
          "        \n",
          "        batch_series = data.get('seriess', [])\n",
          "        if not batch_series:\n",
          "            break\n",
          "            \n",
          "        all_series.extend(batch_series)\n",
          "        print(f\"  Retrieved {len(batch_series)} series (Total: {len(all_series)})\")\n",
          "        \n",
          "        if len(batch_series) < BATCH_LIMIT:\n",
          "            break\n",
          "            \n",
          "        offset += BATCH_LIMIT\n",
          "        batch_count += 1\n",
          "        time.sleep(0.5)\n",
          "        \n",
          "    except Exception as e:\n",
          "        print(f\"Error: {e}\")\n",
          "        break\n",
          "\n",
          "print(f\"\\n✅ Total series collected: {len(all_series)}\")\n",
          "\n",
          "if not all_series:\n",
          "    raise ValueError(f\"No series found for release_id {release_id}\")\n",
          "\n",
          "# Step 3: Fetch observations\n",
          "print(\"\\n\" + \"=\" * 40)\n",
          "print(\"STEP 3: FETCHING OBSERVATIONS\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "def fetch_observations(series_info, api_key_index):\n",
          "    series_id = series_info['id']\n",
          "    obs_url = f\"{FRED_BASE_URL}/series/observations\"\n",
          "    obs_params = {\n",
          "        \"series_id\": series_id,\n",
          "        \"api_key\": api_keys[api_key_index],\n",
          "        \"file_type\": \"json\"\n",
          "    }\n",
          "    \n",
          "    try:\n",
          "        response = requests.get(obs_url, params=obs_params, timeout=30)\n",
          "        response.raise_for_status()\n",
          "        obs_data = response.json()\n",
          "        observations = obs_data.get('observations', [])\n",
          "        \n",
          "        enriched_obs = []\n",
          "        for obs in observations:\n",
          "            if obs['value'] != '.' and obs['value'] != '':\n",
          "                try:\n",
          "                    value = float(obs['value'])\n",
          "                    enriched_obs.append({\n",
          "                        'ObservationDate': obs['date'],\n",
          "                        'SeriesID': series_id,\n",
          "                        'Value': value,\n",
          "                        'Units': series_info.get('units', ''),\n",
          "                        'UnitsShort': series_info.get('units_short', ''),\n",
          "                        'SeasonalAdjustment': series_info.get('seasonal_adjustment', ''),\n",
          "                        'Frequency': series_info.get('frequency', ''),\n",
          "                        'SeriesTitle': series_info.get('title', ''),\n",
          "                        'Notes': series_info.get('notes', ''),\n",
          "                        'ReleaseID': int(release_id),\n",
          "                        'ReleaseName': release_name\n",
          "                    })\n",
          "                except ValueError:\n",
          "                    continue\n",
          "        \n",
          "        return series_id, enriched_obs, None\n",
          "    except Exception as e:\n",
          "        return series_id, [], str(e)\n",
          "\n",
          "all_observations = []\n",
          "errors = []\n",
          "\n",
          "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
          "    futures = {}\n",
          "    for i, series in enumerate(all_series):\n",
          "        future = executor.submit(fetch_observations, series, i % len(api_keys))\n",
          "        futures[future] = series['id']\n",
          "    \n",
          "    for i, future in enumerate(as_completed(futures)):\n",
          "        series_id, observations, error = future.result()\n",
          "        if error:\n",
          "            errors.append((series_id, error))\n",
          "        else:\n",
          "            all_observations.extend(observations)\n",
          "        \n",
          "        if (i + 1) % 100 == 0:\n",
          "            print(f\"  Processed {i + 1}/{len(all_series)} series\")\n",
          "\n",
          "print(f\"\\n✅ Total observations: {len(all_observations)}\")\n",
          "if errors:\n",
          "    print(f\"⚠️  Series with errors: {len(errors)}\")\n",
          "\n",
          "# Step 4: Create DataFrame\n",
          "print(\"\\n\" + \"=\" * 40)\n",
          "print(\"STEP 4: CREATING DATAFRAME\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "schema = StructType([\n",
          "    StructField(\"ObservationDate\", StringType(), True),\n",
          "    StructField(\"SeriesID\", StringType(), True),\n",
          "    StructField(\"Value\", DoubleType(), True),\n",
          "    StructField(\"Units\", StringType(), True),\n",
          "    StructField(\"UnitsShort\", StringType(), True),\n",
          "    StructField(\"SeasonalAdjustment\", StringType(), True),\n",
          "    StructField(\"Frequency\", StringType(), True),\n",
          "    StructField(\"SeriesTitle\", StringType(), True),\n",
          "    StructField(\"Notes\", StringType(), True),\n",
          "    StructField(\"ReleaseID\", IntegerType(), True),\n",
          "    StructField(\"ReleaseName\", StringType(), True)\n",
          "])\n",
          "\n",
          "df = spark.createDataFrame(all_observations, schema)\n",
          "\n",
          "# Add computed columns\n",
          "df = df.withColumn(\"ObservationDate\", to_date(col(\"ObservationDate\"), \"yyyy-MM-dd\")) \\\n",
          "       .withColumn(\"ObservationTimestamp\", to_timestamp(col(\"ObservationDate\"))) \\\n",
          "       .withColumn(\"IngestionTimestamp\", current_timestamp()) \\\n",
          "       .withColumn(\"Source\", lit(\"FRED API\"))\n",
          "\n",
          "record_count = df.count()\n",
          "print(f\"✅ DataFrame created with {record_count:,} records\")\n",
          "\n",
          "# Step 5: Write to ADX\n",
          "print(\"\\n\" + \"=\" * 40)\n",
          "print(\"STEP 5: WRITING TO ADX\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "# Clean table name\n",
          "import re\n",
          "clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', release_name.lower())\n",
          "clean_name = re.sub(r'_+', '_', clean_name).strip('_')\n",
          "table_name = f\"fred_release_{release_id}_{clean_name}\"\n",
          "\n",
          "print(f\"Table: {table_name}\")\n",
          "print(f\"Database: macro-data\")\n",
          "print(f\"Mode: Overwrite\")\n",
          "\n",
          "try:\n",
          "    df.write \\\n",
          "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
          "        .option(\"spark.synapse.linkedService\", \"ADXLinkedService\") \\\n",
          "        .option(\"kustoDatabase\", \"macro-data\") \\\n",
          "        .option(\"kustoTable\", table_name) \\\n",
          "        .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
          "        .mode(\"Overwrite\") \\\n",
          "        .save()\n",
          "    \n",
          "    print(f\"\\n✅ SUCCESS: {record_count:,} records written to {table_name}\")\n",
          "    \n",
          "except Exception as e:\n",
          "    print(f\"\\n❌ ERROR writing to ADX: {e}\")\n",
          "    traceback.print_exc()\n",
          "    raise\n",
          "\n",
          "print(\"\\n\" + \"=\" * 80)\n",
          "print(\"INGESTION COMPLETE\")\n",
          "print(f\"Release: {release_name} (ID: {release_id})\")\n",
          "print(f\"Series: {len(all_series)}\")\n",
          "print(f\"Records: {record_count:,}\")\n",
          "print(f\"Table: {table_name}\")\n",
          "print(\"=\" * 80)"
        ],
        "outputs": [],
        "execution_count": null
      }
    ]
  }
}