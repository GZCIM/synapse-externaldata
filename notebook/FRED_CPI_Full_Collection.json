{
  "etag": "c2003348-0000-0100-0000-68b0561a0000",
  "id": "/subscriptions/6f928fec-8d15-47d7-b27b-be8b568e9789/resourceGroups/MTWS_Synapse/providers/Microsoft.Synapse/workspaces/externaldata/notebooks/FRED_CPI_Full_Collection",
  "name": "FRED_CPI_Full_Collection",
  "properties": {
    "bigDataPool": null,
    "cells": [
      {
        "additionalProperties": null,
        "attachments": null,
        "cellType": "unknown",
        "metadata": {
          "collapsed": false
        },
        "outputs": [],
        "source": [
          "#!/usr/bin/env python3",
          "\"\"\"",
          "Simple Parallel FRED Release to ADX Ingestion ",
          "- Parallel processing with hardcoded API keys",
          "- Limited series for testing (100 series)",
          "- No Key Vault dependency",
          "\"\"\"",
          "",
          "import requests",
          "import pandas as pd",
          "from datetime import datetime",
          "import time",
          "import re",
          "import concurrent.futures",
          "",
          "# =============================================================================",
          "# NOTEBOOK PARAMETERS (Set by Synapse Pipeline)",
          "# =============================================================================",
          "# Get release_id from pipeline parameter or default to CPI",
          "try:",
          "    release_id = int(dbutils.widgets.get(\"release_id\"))",
          "    print(f\"âœ… Using pipeline parameter: release_id = {release_id}\")",
          "except:",
          "    release_id = 10  # Default to CPI for testing",
          "    print(f\"âš ï¸ Using default: release_id = {release_id}\")",
          "",
          "# =============================================================================",
          "# CONFIGURATION",
          "# =============================================================================",
          "ADX_DATABASE = \"macro-data\"",
          "ADX_LINKED_SERVICE = \"ADXLinkedService\"",
          "FRED_BASE_URL = \"https://api.stlouisfed.org/fred\"",
          "",
          "# Fresh API keys for parallel processing",
          "API_KEYS = [",
          "    \"300a685dde47306828b8ff947a527113\",  # meta data collector 3",
          "    \"1211005324f682746cb9b44882837c7b\",  # meta data collector 2  ",
          "    \"ad3badb0ffdaaef1a205e105a14c0fbf\",  # meta data collector 1",
          "    \"15396d5e6513a4242653cdf9f297d57e\"   # meta data collector 4 (new)",
          "]",
          "",
          "# Full production mode - ALL series",
          "SERIES_LIMIT = None  # No limit - get ALL series",
          "",
          "print(\"=\"*80)",
          "print(f\"PARALLEL FRED UNIVERSAL RELEASE TO ADX INGESTION\")",
          "print(f\"Release ID: {release_id}\")",
          "print(f\"Series Limit: {'None (full dataset)' if SERIES_LIMIT is None else SERIES_LIMIT}\")",
          "print(f\"API Keys: {len(API_KEYS)} keys for parallel processing\")",
          "print(\"=\"*80)",
          "",
          "# =============================================================================",
          "# STEP 1: FETCH RELEASE METADATA",
          "# =============================================================================",
          "print(\"\\n[1] FETCHING RELEASE METADATA\")",
          "print(\"-\" * 40)",
          "",
          "release_url = f\"{FRED_BASE_URL}/release\"",
          "params = {",
          "    \"release_id\": release_id,",
          "    \"api_key\": API_KEYS[0],",
          "    \"file_type\": \"json\"",
          "}",
          "",
          "response = requests.get(release_url, params=params)",
          "if response.status_code != 200:",
          "    raise Exception(f\"Failed to fetch release metadata: {response.status_code}\")",
          "",
          "release_data = response.json()",
          "release_info = release_data['releases'][0]",
          "release_name = release_info['name']",
          "release_source = release_info.get('source', 'FRED')",
          "",
          "print(f\"âœ… Release Name: {release_name}\")",
          "print(f\"âœ… Source: {release_source}\")",
          "",
          "# Create table name from release name (sanitized)",
          "table_name_suffix = re.sub(r'[^a-z0-9]+', '_', release_name.lower())",
          "table_name_suffix = re.sub(r'_+', '_', table_name_suffix).strip('_')",
          "table_name = f\"fred_release_{release_id}_{table_name_suffix}\"",
          "",
          "# Truncate if too long",
          "if len(table_name) > 100:",
          "    table_name = table_name[:97] + \"...\"",
          "",
          "print(f\"âœ… Target Table: {table_name}\")",
          "",
          "# =============================================================================",
          "# STEP 2: FETCH SERIES LIST (LIMITED FOR TESTING)",
          "# =============================================================================",
          "print(f\"\\n[2] FETCHING ALL SERIES IN RELEASE\")",
          "print(\"-\" * 40)",
          "",
          "all_series = []",
          "offset = 0",
          "limit = 1000  # API max per request",
          "",
          "while True:",
          "    series_url = f\"{FRED_BASE_URL}/release/series\"",
          "    params = {",
          "        \"release_id\": release_id,",
          "        \"api_key\": API_KEYS[0],",
          "        \"file_type\": \"json\",",
          "        \"limit\": limit,",
          "        \"offset\": offset",
          "    }",
          "    ",
          "    response = requests.get(series_url, params=params)",
          "    if response.status_code != 200:",
          "        raise Exception(f\"Failed to fetch series list: {response.status_code}\")",
          "    ",
          "    series_data = response.json()",
          "    batch = series_data.get('seriess', [])",
          "    ",
          "    if not batch:",
          "        break",
          "        ",
          "    all_series.extend(batch)",
          "    print(f\"  Fetched {len(batch)} series (total: {len(all_series)})\")",
          "    ",
          "    # Apply testing limit if set",
          "    if SERIES_LIMIT and len(all_series) >= SERIES_LIMIT:",
          "        all_series = all_series[:SERIES_LIMIT]",
          "        print(f\"  Limited to {SERIES_LIMIT} series for testing\")",
          "        break",
          "        ",
          "    if len(batch) < limit:",
          "        break",
          "        ",
          "    offset += limit",
          "    time.sleep(0.5)  # Rate limiting",
          "",
          "print(f\"âœ… Total series to process: {len(all_series)}\")",
          "",
          "if len(all_series) == 0:",
          "    raise Exception(f\"No series found for release {release_id}\")",
          "",
          "# Show first few series",
          "print(\"\\nFirst 5 series:\")",
          "for s in all_series[:5]:",
          "    print(f\"  â€¢ {s['id']}: {s['title'][:60]}...\")",
          "",
          "# =============================================================================",
          "# STEP 3: PARALLEL OBSERVATIONS COLLECTION",
          "# =============================================================================",
          "print(f\"\\n[3] PARALLEL OBSERVATIONS COLLECTION\")",
          "print(\"-\" * 40)",
          "print(f\"Using {len(API_KEYS)} API keys for parallel processing\")",
          "",
          "def fetch_series_observations(series_info):",
          "    \"\"\"Fetch observations for a single series with API key rotation\"\"\"",
          "    series_id = series_info['id']",
          "    ",
          "    # Rotate API keys to distribute load",
          "    key_index = hash(series_id) % len(API_KEYS)",
          "    api_key = API_KEYS[key_index]",
          "    ",
          "    obs_url = f\"{FRED_BASE_URL}/series/observations\"",
          "    params = {",
          "        \"series_id\": series_id,",
          "        \"api_key\": api_key,",
          "        \"file_type\": \"json\",",
          "        \"limit\": 100000",
          "    }",
          "    ",
          "    try:",
          "        time.sleep(0.1)  # Reduced delay for faster processing",
          "        response = requests.get(obs_url, params=params)",
          "        if response.status_code == 200:",
          "            obs_data = response.json()",
          "            observations = obs_data.get('observations', [])",
          "            ",
          "            # Add metadata to each observation",
          "            for obs in observations:",
          "                obs['SeriesID'] = series_id",
          "                obs['SeriesTitle'] = series_info.get('title', '')",
          "                obs['Units'] = series_info.get('units', '')",
          "                obs['UnitsShort'] = series_info.get('units_short', '')",
          "                obs['SeasonalAdjustment'] = series_info.get('seasonal_adjustment', '')",
          "                obs['SeasonalAdjustmentShort'] = series_info.get('seasonal_adjustment_short', '')",
          "                obs['Frequency'] = series_info.get('frequency', '')",
          "                obs['FrequencyShort'] = series_info.get('frequency_short', '')",
          "                obs['LastUpdated'] = series_info.get('last_updated', '')",
          "                obs['Notes'] = series_info.get('notes', '')[:500] if series_info.get('notes') else ''",
          "                obs['ReleaseID'] = release_id",
          "                obs['ReleaseName'] = release_name",
          "                obs['Source'] = release_source",
          "                ",
          "            return observations",
          "        else:",
          "            return []",
          "    except Exception as e:",
          "        print(f\"âš ï¸ Failed {series_id}: {str(e)[:30]}\")",
          "        return []",
          "",
          "# Parallel processing with ThreadPoolExecutor",
          "all_observations = []",
          "failed_series = []",
          "",
          "print(f\"Processing {len(all_series)} series in parallel...\")",
          "start_time = time.time()",
          "",
          "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:  # Full parallel power",
          "    # Submit all tasks",
          "    future_to_series = {",
          "        executor.submit(fetch_series_observations, series): series['id'] ",
          "        for series in all_series",
          "    }",
          "    ",
          "    # Collect results as they complete",
          "    completed = 0",
          "    for future in concurrent.futures.as_completed(future_to_series):",
          "        series_id = future_to_series[future]",
          "        try:",
          "            observations = future.result()",
          "            all_observations.extend(observations)",
          "            completed += 1",
          "            ",
          "            if completed % 10 == 0:",
          "                print(f\"  Progress: {completed}/{len(all_series)} series ({len(all_observations)} observations)\")",
          "                ",
          "        except Exception as e:",
          "            failed_series.append(series_id)",
          "            completed += 1",
          "",
          "processing_time = time.time() - start_time",
          "",
          "print(f\"\\nâœ… Collection completed in {processing_time:.1f} seconds\")",
          "print(f\"âœ… Observations collected: {len(all_observations):,}\")",
          "print(f\"âœ… Processing speed: {len(all_series)/processing_time:.1f} series/second\")",
          "",
          "if failed_series:",
          "    print(f\"âš ï¸ Failed series: {len(failed_series)}\")",
          "",
          "# =============================================================================",
          "# STEP 4: PREPARE DATAFRAME",
          "# =============================================================================",
          "print(\"\\n[4] PREPARING DATAFRAME\")",
          "print(\"-\" * 40)",
          "",
          "if len(all_observations) == 0:",
          "    raise Exception(\"No observations collected\")",
          "",
          "# Convert to DataFrame",
          "df = pd.DataFrame(all_observations)",
          "",
          "# Rename columns to match ADX schema",
          "column_mapping = {",
          "    'date': 'ObservationDate',",
          "    'value': 'Value'",
          "}",
          "",
          "df = df.rename(columns=column_mapping)",
          "",
          "# Convert data types",
          "df['ObservationDate'] = pd.to_datetime(df['ObservationDate'])",
          "df['Value'] = pd.to_numeric(df['Value'], errors='coerce')",
          "",
          "# Add timestamp column",
          "df['ObservationTimestamp'] = df['ObservationDate'].apply(",
          "    lambda x: datetime.combine(x, datetime.min.time())",
          ")",
          "",
          "# Add ingestion timestamp",
          "df['IngestionTimestamp'] = datetime.utcnow()",
          "",
          "# Select and order columns for ADX",
          "adx_columns = [",
          "    'ObservationTimestamp',",
          "    'ObservationDate',",
          "    'SeriesID',",
          "    'Value',",
          "    'Units',",
          "    'UnitsShort',",
          "    'SeasonalAdjustment',",
          "    'SeasonalAdjustmentShort',",
          "    'Frequency',",
          "    'FrequencyShort',",
          "    'LastUpdated',",
          "    'SeriesTitle',",
          "    'Notes',",
          "    'ReleaseID',",
          "    'ReleaseName',",
          "    'Source',",
          "    'IngestionTimestamp'",
          "]",
          "",
          "# Only include columns that exist",
          "final_columns = [col for col in adx_columns if col in df.columns]",
          "df_final = df[final_columns]",
          "",
          "# Remove any completely null rows",
          "df_final = df_final.dropna(subset=['Value', 'ObservationDate'])",
          "",
          "print(f\"âœ… DataFrame shape: {df_final.shape}\")",
          "print(f\"âœ… Date range: {df_final['ObservationDate'].min()} to {df_final['ObservationDate'].max()}\")",
          "print(f\"âœ… Unique series: {df_final['SeriesID'].nunique()}\")",
          "print(f\"âœ… Records per series: {len(df_final) // df_final['SeriesID'].nunique():.0f} average\")",
          "",
          "# Performance metrics",
          "total_runtime = processing_time",
          "print(f\"\\nðŸ“Š PERFORMANCE METRICS\")",
          "print(f\"âœ… Total runtime: {total_runtime:.1f} seconds ({total_runtime/60:.1f} minutes)\")",
          "print(f\"âœ… Series/second: {len(all_series)/total_runtime:.1f}\")",
          "print(f\"âœ… Records/second: {len(df_final)/total_runtime:.0f}\")",
          "",
          "# =============================================================================",
          "# STEP 5: CONVERT TO SPARK DATAFRAME",
          "# =============================================================================",
          "print(\"\\n[5] CONVERTING TO SPARK DATAFRAME\")",
          "print(\"-\" * 40)",
          "",
          "spark_df = spark.createDataFrame(df_final)",
          "print(f\"âœ… Spark DataFrame created with {spark_df.count():,} rows\")",
          "",
          "# =============================================================================",
          "# STEP 6: WRITE TO ADX WITH TABLE CREATION",
          "# =============================================================================",
          "print(\"\\n[6] WRITING TO AZURE DATA EXPLORER\")",
          "print(\"-\" * 40)",
          "print(f\"Target Database: {ADX_DATABASE}\")",
          "print(f\"Target Table: {table_name}\")",
          "print(f\"Using LinkedService: {ADX_LINKED_SERVICE}\")",
          "",
          "try:",
          "    spark_df.write \\",
          "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\",
          "        .option(\"spark.synapse.linkedService\", ADX_LINKED_SERVICE) \\",
          "        .option(\"kustoDatabase\", ADX_DATABASE) \\",
          "        .option(\"kustoTable\", table_name) \\",
          "        .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\",
          "        .mode(\"Append\") \\",
          "        .save()",
          "    ",
          "    print(\"âœ… Data successfully written to ADX!\")",
          "    ",
          "except Exception as e:",
          "    print(f\"âŒ ADX write failed: {str(e)}\")",
          "    raise",
          "",
          "# =============================================================================",
          "# STEP 7: VERIFY DATA IN ADX",
          "# =============================================================================",
          "print(\"\\n[7] VERIFYING DATA IN ADX\")",
          "print(\"-\" * 40)",
          "",
          "try:",
          "    # Read back from ADX to verify",
          "    verify_query = f\"{table_name} | summarize Count=count(), MinDate=min(ObservationDate), MaxDate=max(ObservationDate), Series=dcount(SeriesID)\"",
          "    ",
          "    verify_df = spark.read \\",
          "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\",
          "        .option(\"spark.synapse.linkedService\", ADX_LINKED_SERVICE) \\",
          "        .option(\"kustoDatabase\", ADX_DATABASE) \\",
          "        .option(\"kustoQuery\", verify_query) \\",
          "        .load()",
          "    ",
          "    print(\"Verification summary:\")",
          "    verify_df.show()",
          "    ",
          "except Exception as e:",
          "    print(f\"âš ï¸ Verification query failed: {str(e)[:100]}\")",
          "",
          "# =============================================================================",
          "# SUMMARY",
          "# =============================================================================",
          "print(\"\\n\" + \"=\"*80)",
          "print(\"PARALLEL INGESTION COMPLETE\")",
          "print(\"=\"*80)",
          "print(f\"âœ… Release: {release_name} (ID: {release_id})\")",
          "print(f\"âœ… Table: {table_name}\")",
          "print(f\"âœ… Database: {ADX_DATABASE}\")",
          "print(f\"âœ… Records: {len(df_final):,}\")",
          "print(f\"âœ… Series: {df_final['SeriesID'].nunique()}\")",
          "print(f\"âœ… Runtime: {total_runtime:.1f} seconds ({total_runtime/60:.1f} minutes)\")",
          "print(f\"âœ… Speed: {len(all_series)/total_runtime:.1f} series/second\")",
          "print()",
          "print(\"ðŸš€ PARALLEL PROCESSING SUCCESS!\")",
          "print(f\"  â€¢ {len(API_KEYS)} API keys used\")",
          "print(f\"  â€¢ 8 concurrent workers\")",
          "print(f\"  â€¢ Series limit: {SERIES_LIMIT} (testing mode)\")",
          "print()",
          "print(\"ðŸ“Š Ready for production with full series count!\")",
          "print(f\"   Estimated full CPI time: ~{(4609/len(all_series)) * (total_runtime/60):.0f} minutes\")"
        ]
      }
    ],
    "description": null,
    "entityState": null,
    "folder": null,
    "metadata": {
      "a365ComputeOptions": null,
      "kernelspec": null,
      "languageInfo": {
        "additionalProperties": null,
        "codemirrorMode": null,
        "name": "python"
      },
      "sessionKeepAliveTimeout": 0
    },
    "nbformat": 4,
    "nbformatMinor": 0,
    "renameOperationDetails": null,
    "sessionProperties": null,
    "targetSparkConfiguration": null
  },
  "resourceGroup": "MTWS_Synapse",
  "type": "Microsoft.Synapse/workspaces/notebooks"
}
