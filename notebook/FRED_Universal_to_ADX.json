{
    "name": "FRED_Universal_to_ADX",
    "properties": {
        "nbformat": 4,
        "nbformat_minor": 2,
        "bigDataPool": {
            "referenceName": "spack2",
            "type": "BigDataPoolReference"
        },
        "sessionProperties": {
            "driverMemory": "56g",
            "driverCores": 8,
            "executorMemory": "56g",
            "executorCores": 8,
            "numExecutors": 2,
            "conf": {
                "spark.dynamicAllocation.enabled": "false",
                "spark.dynamicAllocation.minExecutors": "2",
                "spark.dynamicAllocation.maxExecutors": "2",
                "spark.autotune.trackingId": "00000000-0000-0000-0000-000000000000"
            }
        },
        "metadata": {
            "saveOutput": true,
            "enableDebugMode": false,
            "kernelspec": {
                "name": "synapse_pyspark",
                "display_name": "Synapse PySpark"
            },
            "language_info": {
                "name": "python"
            },
            "a365ComputeOptions": {
                "id": "/subscriptions/6f928fec-8d15-47d7-b27b-be8b568e9789/resourceGroups/MTWS_Synapse/providers/Microsoft.Synapse/workspaces/externaldata/bigDataPools/spack2",
                "name": "spack2",
                "type": "Spark",
                "endpoint": "https://externaldata.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spack2",
                "auth": {
                    "type": "AAD",
                    "authResource": "https://dev.azuresynapse.net"
                },
                "sparkVersion": "3.3",
                "nodeCount": 3,
                "cores": 8,
                "memory": 56,
                "automaticScaleJobs": false
            },
            "sessionKeepAliveTimeout": 30
        },
        "cells": [
            {
                "cell_type": "code",
                "metadata": {},
                "source": [
                    "#!/usr/bin/env python3\n",
                    "\"\"\"\n",
                    "FRED UNIVERSAL INGESTION TO ADX - FIXED PAGINATION VERSION\n",
                    "\"\"\"\n",
                    "\n",
                    "import json\n",
                    "import time\n",
                    "import requests\n",
                    "from datetime import datetime, timezone\n",
                    "from pyspark.sql import SparkSession\n",
                    "from pyspark.sql.functions import col, lit, to_timestamp, to_date, current_timestamp\n",
                    "from pyspark.sql.types import *\n",
                    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
                    "import traceback\n",
                    "\n",
                    "print(\"=\" * 70)\n",
                    "print(\"FRED UNIVERSAL INGESTION TO ADX - PIPELINE VERSION\")\n",
                    "print(\"=\" * 70)\n",
                    "\n",
                    "# Get parameters from pipeline\n",
                    "try:\n",
                    "    dbutils.widgets.text(\"release_id\", \"10\")\n",
                    "    release_id = dbutils.widgets.get(\"release_id\")\n",
                    "    print(f\"Release ID from pipeline: {release_id}\")\n",
                    "except NameError:\n",
                    "    # dbutils not available, use default\n",
                    "    import os\n",
                    "    release_id = os.environ.get('RELEASE_ID', '10')\n",
                    "    print(f\"Release ID from environment: {release_id}\")\n",
                    "\n",
                    "# API Keys - Fresh keys with high limits\n",
                    "api_keys = [\n",
                    "    \"300a685dde47306828b8ff947a527113\",  # meta data collector 3\n",
                    "    \"1211005324f682746cb9b44882837c7b\",  # meta data collector 2  \n",
                    "    \"ad3badb0ffdaaef1a205e105a14c0fbf\",  # meta data collector 1\n",
                    "    \"21acd97c4988e53af02e98587d5424d0\"   # original key\n",
                    "]\n",
                    "\n",
                    "# Configuration\n",
                    "batch_limit = 1000  # FRED API max limit\n",
                    "max_workers = 4\n",
                    "\n",
                    "# STEP 1: GET RELEASE INFO AND SERIES COUNT - FIXED PAGINATION\n",
                    "print(f\"\\n1. Getting release info for release_id: {release_id}...\")\n",
                    "release_url = f\"https://api.stlouisfed.org/fred/release\"\n",
                    "release_params = {\n",
                    "    \"release_id\": release_id,\n",
                    "    \"api_key\": api_keys[0],\n",
                    "    \"file_type\": \"json\"\n",
                    "}\n",
                    "\n",
                    "try:\n",
                    "    release_response = requests.get(release_url, params=release_params, timeout=30)\n",
                    "    release_response.raise_for_status()\n",
                    "    release_data = release_response.json()\n",
                    "    release_info = release_data['releases'][0]\n",
                    "    release_name = release_info['name']\n",
                    "    print(f\"   Release Name: {release_name}\")\n",
                    "except Exception as e:\n",
                    "    print(f\"   Failed to get release info: {e}\")\n",
                    "    release_name = f\"Release_{release_id}\"\n",
                    "\n",
                    "# STEP 2: GET ALL SERIES WITH PROPER PAGINATION\n",
                    "print(f\"\\n2. Fetching ALL series for release (using proper pagination)...\")\n",
                    "all_series = []\n",
                    "offset = 0\n",
                    "batch_count = 0\n",
                    "max_batches = 50  # Safety limit - 50 * 1000 = 50,000 series max\n",
                    "\n",
                    "while batch_count < max_batches:\n",
                    "    series_url = f\"https://api.stlouisfed.org/fred/release/series\"\n",
                    "    params = {\n",
                    "        \"release_id\": release_id,\n",
                    "        \"api_key\": api_keys[batch_count % len(api_keys)],  # Rotate API keys\n",
                    "        \"file_type\": \"json\",\n",
                    "        \"limit\": batch_limit,\n",
                    "        \"offset\": offset\n",
                    "    }\n",
                    "    \n",
                    "    print(f\"   Fetching batch {batch_count + 1} (offset: {offset}, limit: {batch_limit})...\")\n",
                    "    \n",
                    "    try:\n",
                    "        response = requests.get(series_url, params=params, timeout=30)\n",
                    "        response.raise_for_status()\n",
                    "        data = response.json()\n",
                    "        \n",
                    "        batch_series = data.get('seriess', [])\n",
                    "        if not batch_series:\n",
                    "            print(f\"   No more series found. Total collected: {len(all_series)}\")\n",
                    "            break\n",
                    "            \n",
                    "        all_series.extend(batch_series)\n",
                    "        print(f\"   Batch {batch_count + 1}: Retrieved {len(batch_series)} series (Total: {len(all_series)})\")\n",
                    "        \n",
                    "        # Check if we got less than limit (means we're at the end)\n",
                    "        if len(batch_series) < batch_limit:\n",
                    "            print(f\"   Reached end of series. Total collected: {len(all_series)}\")\n",
                    "            break\n",
                    "            \n",
                    "        offset += batch_limit\n",
                    "        batch_count += 1\n",
                    "        time.sleep(0.5)  # Rate limiting\n",
                    "        \n",
                    "    except requests.exceptions.HTTPError as e:\n",
                    "        if e.response.status_code == 400:\n",
                    "            print(f\"   400 Error - likely reached end of data. Total series: {len(all_series)}\")\n",
                    "            break\n",
                    "        else:\n",
                    "            print(f\"   HTTP Error: {e}\")\n",
                    "            break\n",
                    "    except Exception as e:\n",
                    "        print(f\"   Error in batch {batch_count + 1}: {e}\")\n",
                    "        break\n",
                    "\n",
                    "print(f\"\\n   ✅ SUCCESSFULLY COLLECTED ALL {len(all_series)} SERIES\")\n",
                    "\n",
                    "# STEP 3: FETCH OBSERVATIONS FOR ALL SERIES\n",
                    "print(f\"\\n3. Fetching observations for {len(all_series)} series...\")\n",
                    "\n",
                    "def fetch_series_observations(series_info, api_key_index):\n",
                    "    series_id = series_info['id']\n",
                    "    series_title = series_info.get('title', '')\n",
                    "    units = series_info.get('units', '')\n",
                    "    units_short = series_info.get('units_short', '')\n",
                    "    seasonal_adjustment = series_info.get('seasonal_adjustment', '')\n",
                    "    frequency = series_info.get('frequency', '')\n",
                    "    notes = series_info.get('notes', '')\n",
                    "    \n",
                    "    obs_url = f\"https://api.stlouisfed.org/fred/series/observations\"\n",
                    "    obs_params = {\n",
                    "        \"series_id\": series_id,\n",
                    "        \"api_key\": api_keys[api_key_index],\n",
                    "        \"file_type\": \"json\"\n",
                    "    }\n",
                    "    \n",
                    "    max_retries = 3\n",
                    "    for attempt in range(max_retries):\n",
                    "        try:\n",
                    "            obs_response = requests.get(obs_url, params=obs_params, timeout=30)\n",
                    "            obs_response.raise_for_status()\n",
                    "            obs_data = obs_response.json()\n",
                    "            \n",
                    "            observations = obs_data.get('observations', [])\n",
                    "            \n",
                    "            # Enrich observations with metadata\n",
                    "            enriched_obs = []\n",
                    "            for obs in observations:\n",
                    "                try:\n",
                    "                    value = float(obs['value']) if obs['value'] != '.' else None\n",
                    "                    if value is not None:\n",
                    "                        enriched_obs.append({\n",
                    "                            'ObservationDate': obs['date'],\n",
                    "                            'SeriesID': series_id,\n",
                    "                            'Value': value,\n",
                    "                            'Units': units,\n",
                    "                            'UnitsShort': units_short,\n",
                    "                            'SeasonalAdjustment': seasonal_adjustment,\n",
                    "                            'Frequency': frequency,\n",
                    "                            'SeriesTitle': series_title,\n",
                    "                            'Notes': notes,\n",
                    "                            'ReleaseID': int(release_id),\n",
                    "                            'ReleaseName': release_name\n",
                    "                        })\n",
                    "                except ValueError:\n",
                    "                    continue\n",
                    "            \n",
                    "            return series_id, enriched_obs, None\n",
                    "            \n",
                    "        except Exception as e:\n",
                    "            if attempt == max_retries - 1:\n",
                    "                return series_id, [], str(e)\n",
                    "            time.sleep(2 ** attempt)\n",
                    "    \n",
                    "    return series_id, [], \"Max retries exceeded\"\n",
                    "\n",
                    "# Parallel fetching\n",
                    "all_observations = []\n",
                    "errors = []\n",
                    "processed_count = 0\n",
                    "\n",
                    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
                    "    futures = {}\n",
                    "    for i, series in enumerate(all_series):\n",
                    "        api_key_index = i % len(api_keys)\n",
                    "        future = executor.submit(fetch_series_observations, series, api_key_index)\n",
                    "        futures[future] = series['id']\n",
                    "    \n",
                    "    for future in as_completed(futures):\n",
                    "        series_id, observations, error = future.result()\n",
                    "        processed_count += 1\n",
                    "        \n",
                    "        if error:\n",
                    "            errors.append((series_id, error))\n",
                    "            print(f\"   [{processed_count}/{len(all_series)}] Error fetching {series_id}: {error}\")\n",
                    "        else:\n",
                    "            all_observations.extend(observations)\n",
                    "            if processed_count % 100 == 0:\n",
                    "                print(f\"   [{processed_count}/{len(all_series)}] Processed {processed_count} series, {len(all_observations)} observations\")\n",
                    "\n",
                    "print(f\"\\n   ✅ Fetched {len(all_observations)} observations from {len(all_series)} series\")\n",
                    "if errors:\n",
                    "    print(f\"   ⚠️  {len(errors)} series had errors\")\n",
                    "\n",
                    "# STEP 4: CREATE SPARK DATAFRAME\n",
                    "print(f\"\\n4. Creating Spark DataFrame...\")\n",
                    "\n",
                    "schema = StructType([\n",
                    "    StructField(\"ObservationDate\", StringType(), True),\n",
                    "    StructField(\"SeriesID\", StringType(), True),\n",
                    "    StructField(\"Value\", DoubleType(), True),\n",
                    "    StructField(\"Units\", StringType(), True),\n",
                    "    StructField(\"UnitsShort\", StringType(), True),\n",
                    "    StructField(\"SeasonalAdjustment\", StringType(), True),\n",
                    "    StructField(\"Frequency\", StringType(), True),\n",
                    "    StructField(\"SeriesTitle\", StringType(), True),\n",
                    "    StructField(\"Notes\", StringType(), True),\n",
                    "    StructField(\"ReleaseID\", IntegerType(), True),\n",
                    "    StructField(\"ReleaseName\", StringType(), True)\n",
                    "])\n",
                    "\n",
                    "df = spark.createDataFrame(all_observations, schema)\n",
                    "\n",
                    "# Add timestamps and convert date\n",
                    "df = df.withColumn(\"ObservationDate\", to_date(col(\"ObservationDate\"), \"yyyy-MM-dd\")) \\\n",
                    "       .withColumn(\"ObservationTimestamp\", to_timestamp(col(\"ObservationDate\"))) \\\n",
                    "       .withColumn(\"IngestionTimestamp\", current_timestamp()) \\\n",
                    "       .withColumn(\"Source\", lit(\"FRED API\"))\n",
                    "\n",
                    "print(f\"   DataFrame created with {df.count()} records\")\n",
                    "df.printSchema()\n",
                    "\n",
                    "# STEP 5: WRITE TO ADX\n",
                    "print(f\"\\n5. Writing to Azure Data Explorer...\")\n",
                    "\n",
                    "# Clean table name\n",
                    "table_name = f\"fred_release_{release_id}_{release_name.lower().replace(' ', '_').replace('(', '').replace(')', '')}\"\n",
                    "print(f\"   Table: {table_name}\")\n",
                    "\n",
                    "try:\n",
                    "    df.write \\\n",
                    "        .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
                    "        .option(\"spark.synapse.linkedService\", \"ADXLinkedService\") \\\n",
                    "        .option(\"kustoDatabase\", \"macro-data\") \\\n",
                    "        .option(\"kustoTable\", table_name) \\\n",
                    "        .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
                    "        .mode(\"Append\") \\\n",
                    "        .save()\n",
                    "    \n",
                    "    print(f\"   ✅ Successfully wrote {df.count()} records to ADX table {table_name}\")\n",
                    "    \n",
                    "except Exception as e:\n",
                    "    print(f\"   ❌ Failed to write to ADX: {e}\")\n",
                    "    print(\"   Full error:\")\n",
                    "    traceback.print_exc()\n",
                    "\n",
                    "print(\"\\n\" + \"=\" * 70)\n",
                    "print(\"INGESTION COMPLETE\")\n",
                    "print(f\"Series: {len(all_series)}\")\n",
                    "print(f\"Records: {len(all_observations)}\")\n",
                    "print(\"=\" * 70)"
                ],
                "outputs": [],
                "execution_count": null
            }
        ]
    }
}