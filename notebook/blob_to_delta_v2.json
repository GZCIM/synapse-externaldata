{
  "name": "blob_to_delta_v2",
  "properties": {
    "cells": [
      {
        "cell_type": "markdown",
        "source": [
          "# Blob to Delta Lake Transformer v2\n",
          "\n",
          "**CRITICAL FIX**: Uses `release_date` (FRED publication date) as the `vintage_date`\n",
          "\n",
          "- Source: Raw parquet files from blob storage\n",
          "- Target: Delta Lake with proper vintage tracking\n",
          "- Partitioning: (year, month) for query performance\n",
          "- Z-ordering: series_id, release_id"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "source": [
          "from pyspark.sql import SparkSession\n",
          "from pyspark.sql.functions import (\n",
          "    col, lit, substring, year, month, count, countDistinct,\n",
          "    min as spark_min, max as spark_max\n",
          ")\n",
          "from delta.tables import DeltaTable\n",
          "import time\n",
          "\n",
          "# Configuration\n",
          "STORAGE_ACCOUNT = \"gzcstorageaccount\"\n",
          "CONTAINER = \"macroeconomic-maintained-series\"\n",
          "COLLECTION_DATE = \"2025-11-21\"\n",
          "\n",
          "BLOB_PREFIX = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/US_Fred_Data/raw/{COLLECTION_DATE}/\"\n",
          "DELTA_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/US_Fred_Data/series_observations_v2\"\n",
          "\n",
          "print(\"=\"*80)\n",
          "print(\"BLOB TO DELTA TRANSFORMER v2\")\n",
          "print(\"=\"*80)\n",
          "print(f\"Collection Date: {COLLECTION_DATE}\")\n",
          "print(f\"Source: {BLOB_PREFIX}\")\n",
          "print(f\"Target: {DELTA_PATH}\")\n",
          "print()\n",
          "print(\"KEY FIX: Using release_date (FRED publication date) as vintage_date\")"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "source": [
          "# Step 1: Read all parquet files\n",
          "print(\"\\nStep 1: Reading blob files...\")\n",
          "start_time = time.time()\n",
          "\n",
          "df = spark.read.parquet(f\"{BLOB_PREFIX}*.parquet\")\n",
          "total_rows = df.count()\n",
          "\n",
          "print(f\"✅ Read {total_rows:,} rows from blob storage\")\n",
          "print(f\"   Columns: {df.columns}\")"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "source": [
          "# Step 2: Transform columns - USE release_date AS vintage_date\n",
          "print(\"\\nStep 2: Transforming columns...\")\n",
          "print(\"   CRITICAL: release_date becomes vintage_date (FRED publication date)\")\n",
          "print(\"            old vintage_date becomes collection_date (when we pulled data)\")\n",
          "\n",
          "# Check what we have\n",
          "print(f\"\\n   Source columns: {df.columns}\")\n",
          "\n",
          "# Sample release_date values before transform\n",
          "print(\"\\n   Sample release_date values (before):\")\n",
          "df.select(\"release_id\", \"release_date\").distinct().show(5, truncate=False)\n",
          "\n",
          "# Transform: rename columns correctly\n",
          "# vintage_date (collection date 2025-11-21) -> collection_date  \n",
          "# release_date (FRED pub date) -> vintage_date\n",
          "transformed_df = df \\\n",
          "    .withColumnRenamed(\"vintage_date\", \"collection_date\") \\\n",
          "    .withColumnRenamed(\"release_date\", \"vintage_date\") \\\n",
          "    .withColumn(\"year\", substring(col(\"date\"), 1, 4).cast(\"int\")) \\\n",
          "    .withColumn(\"month\", substring(col(\"date\"), 6, 2).cast(\"int\"))\n",
          "\n",
          "print(f\"\\n   Transformed columns: {transformed_df.columns}\")\n",
          "\n",
          "# Sample vintage_date values after transform\n",
          "print(\"\\n   Sample vintage_date values (after - should show FRED dates):\")\n",
          "transformed_df.select(\"release_id\", \"vintage_date\").distinct().show(10, truncate=False)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "source": [
          "# Step 3: Deduplicate\n",
          "print(\"\\nStep 3: Deduplicating on (series_id, date)...\")\n",
          "\n",
          "before_dedup = transformed_df.count()\n",
          "deduped_df = transformed_df.dropDuplicates([\"series_id\", \"date\"])\n",
          "after_dedup = deduped_df.count()\n",
          "duplicates_removed = before_dedup - after_dedup\n",
          "\n",
          "print(f\"✅ Removed {duplicates_removed:,} duplicates\")\n",
          "print(f\"   Final rows: {after_dedup:,}\")"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "source": [
          "# Step 4: Quality Control\n",
          "print(\"\\nStep 4: Quality control checks...\")\n",
          "\n",
          "# Check required columns\n",
          "required_cols = [\"series_id\", \"date\", \"value\", \"release_id\", \"vintage_date\",\n",
          "                 \"collection_date\", \"year\", \"month\"]\n",
          "missing_cols = [c for c in required_cols if c not in deduped_df.columns]\n",
          "if missing_cols:\n",
          "    print(f\"❌ Missing columns: {missing_cols}\")\n",
          "    raise Exception(f\"Missing required columns: {missing_cols}\")\n",
          "else:\n",
          "    print(f\"✅ Schema validation passed\")\n",
          "\n",
          "# Check nulls\n",
          "null_counts = deduped_df.select(\n",
          "    col(\"series_id\").isNull().cast(\"int\").alias(\"series_id_nulls\"),\n",
          "    col(\"date\").isNull().cast(\"int\").alias(\"date_nulls\"),\n",
          "    col(\"vintage_date\").isNull().cast(\"int\").alias(\"vintage_date_nulls\")\n",
          ").groupBy().sum().collect()[0]\n",
          "\n",
          "print(f\"   Null check: series_id={null_counts[0]}, date={null_counts[1]}, vintage_date={null_counts[2]}\")\n",
          "\n",
          "# Check vintage_date diversity (CRITICAL!)\n",
          "unique_vintages = deduped_df.select(\"vintage_date\").distinct().count()\n",
          "print(f\"   Unique vintage_dates: {unique_vintages}\")\n",
          "\n",
          "if unique_vintages < 5:\n",
          "    print(f\"⚠️  WARNING: Only {unique_vintages} unique vintage dates - transformation may have failed!\")\n",
          "else:\n",
          "    print(f\"✅ Multiple vintage dates confirmed - transformation correct!\")"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "source": [
          "# Step 5: Write to Delta Lake\n",
          "print(f\"\\nStep 5: Writing to Delta Lake...\")\n",
          "print(f\"Target: {DELTA_PATH}\")\n",
          "print(f\"Partitioning: (year, month)\")\n",
          "\n",
          "write_start = time.time()\n",
          "\n",
          "# Write with overwrite mode\n",
          "deduped_df.write \\\n",
          "    .format(\"delta\") \\\n",
          "    .mode(\"overwrite\") \\\n",
          "    .partitionBy(\"year\", \"month\") \\\n",
          "    .option(\"overwriteSchema\", \"true\") \\\n",
          "    .save(DELTA_PATH)\n",
          "\n",
          "write_duration = time.time() - write_start\n",
          "print(f\"✅ Write complete in {write_duration:.2f}s\")"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "source": [
          "# Step 6: Optimize with Z-ordering\n",
          "print(f\"\\nStep 6: Optimizing with Z-ordering...\")\n",
          "\n",
          "try:\n",
          "    delta_table = DeltaTable.forPath(spark, DELTA_PATH)\n",
          "    delta_table.optimize().executeZOrderBy(\"series_id\", \"release_id\")\n",
          "    print(f\"✅ Z-ordering complete\")\n",
          "except Exception as e:\n",
          "    print(f\"⚠️ Z-ordering skipped: {e}\")"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "source": [
          "# Step 7: Validate output\n",
          "print(f\"\\nStep 7: Validating output...\")\n",
          "\n",
          "result_df = spark.read.format(\"delta\").load(DELTA_PATH)\n",
          "final_count = result_df.count()\n",
          "\n",
          "# Vintage date distribution\n",
          "print(\"\\nVintage date distribution (top 15):\")\n",
          "vintage_stats = result_df.groupBy(\"vintage_date\").agg(\n",
          "    count(\"*\").alias(\"count\"),\n",
          "    countDistinct(\"release_id\").alias(\"releases\"),\n",
          "    countDistinct(\"series_id\").alias(\"series\")\n",
          ").orderBy(col(\"count\").desc()).limit(15)\n",
          "\n",
          "vintage_stats.show(truncate=False)\n",
          "\n",
          "# Summary\n",
          "total_duration = time.time() - start_time\n",
          "total_vintages = result_df.select(\"vintage_date\").distinct().count()\n",
          "total_releases = result_df.select(\"release_id\").distinct().count()\n",
          "total_series = result_df.select(\"series_id\").distinct().count()\n",
          "\n",
          "print(\"\\n\" + \"=\"*80)\n",
          "print(\"TRANSFORMATION COMPLETE\")\n",
          "print(\"=\"*80)\n",
          "print(f\"Total rows: {final_count:,}\")\n",
          "print(f\"Unique vintage dates: {total_vintages}\")\n",
          "print(f\"Unique releases: {total_releases}\")\n",
          "print(f\"Unique series: {total_series:,}\")\n",
          "print(f\"Total duration: {total_duration:.2f}s\")\n",
          "print()\n",
          "print(f\"✅ Delta Lake ready at: {DELTA_PATH}\")\n",
          "print(f\"✅ vintage_date = FRED release dates (NOT collection date)\")\n",
          "print(\"=\"*80)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
      }
    ],
    "metadata": {
      "kernelspec": {
        "name": "synapse_pyspark",
        "display_name": "Synapse PySpark"
      },
      "language_info": {
        "name": "python"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 2
  }
}
